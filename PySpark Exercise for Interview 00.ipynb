{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a238c5-ce81-4f7b-ba11-4c4dc1392bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a dataframe in Pyspark for below table and show the result.\n",
    "\n",
    "team\tpoints\tmarks\n",
    "a\t      1\t     10\n",
    "b\t      2\t     5\n",
    "c\t      2\t     5\n",
    "a\t      1\t     10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52906e1-c6c2-4f0b-84f9-f069ee27ef8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n|team|points|marks|\n+----+------+-----+\n|   a|     1|   10|\n|   a|     2|    5|\n|   c|     2|    5|\n|   a|     1|   10|\n+----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Interview\").getOrCreate()\n",
    "column = [\"team\",\t\"points\",\t\"marks\"]\n",
    "data = [(\"a\", 1, 10),(\"a\", 2, 5),(\"c\", 2, 5),(\"a\", 1, 10)]\n",
    "df = spark.createDataFrame(data,column)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce46265-58c1-4c69-ab3b-787d79a904cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n|team|points|marks|\n+----+------+-----+\n|   a|     1|   10|\n|   a|     2|    5|\n|   c|     2|    5|\n+----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=None)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b742c5-8d4f-4575-bf7b-e34fda3140dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n|team|points|marks|\n+----+------+-----+\n|   a|     1|   10|\n|   a|     2|    5|\n|   c|     2|    5|\n|   a|     1|   10|\n+----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc504e74-4842-41e6-b363-92d213c15b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1:\n+-----+-------+---+\n|empid|   name|age|\n+-----+-------+---+\n|    1|  akash| 25|\n|    2|saurabh| 29|\n+-----+-------+---+\n\ndf2:\n+-----+-------+---+------+\n|empid|   name|age|weight|\n+-----+-------+---+------+\n|    3|   raju| 25|    59|\n|    2|saurabh| 29|    72|\n+-----+-------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"UnionOrJoinExample\").getOrCreate()\n",
    "\n",
    "# Create df1\n",
    "data1 = [(1, \"akash\", 25), (2, \"saurabh\", 29)]\n",
    "columns1 = [\"empid\", \"name\", \"age\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "# Create df2\n",
    "data2 = [(3, \"raju\", 25, 59), (2, \"saurabh\", 29, 72)]\n",
    "columns2 = [\"empid\", \"name\", \"age\", \"weight\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Show the DataFrames\n",
    "print(\"df1:\")\n",
    "df1.show()\n",
    "\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf640a9-ca4d-49a3-95e6-c9d342b4563c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined DataFrame:\n+-----+-------+----+-------+----+------+\n|empid|   name| age|   name| age|weight|\n+-----+-------+----+-------+----+------+\n|    1|  akash|  25|   NULL|NULL|  NULL|\n|    2|saurabh|  29|saurabh|  29|    72|\n|    3|   NULL|NULL|   raju|  25|    59|\n+-----+-------+----+-------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform a full outer join on empid\n",
    "df_joined = df1.join(df2, on=\"empid\", how=\"full\")\n",
    "\n",
    "# Show the joined DataFrame\n",
    "print(\"Joined DataFrame:\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da03e835-67b6-4ebf-869f-f54e05283343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output DataFrame (df_out):\n+-----+-------+---+------+\n|empid|   name|age|weight|\n+-----+-------+---+------+\n|    1|  akash| 25|  NULL|\n|    2|saurabh| 29|    72|\n|    3|   raju| 25|    59|\n+-----+-------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Select and rename columns\n",
    "df_out = df_joined.select(\n",
    "    \"empid\",\n",
    "    coalesce(df1[\"name\"], df2[\"name\"]).alias(\"name\"),\n",
    "    coalesce(df1[\"age\"], df2[\"age\"]).alias(\"age\"),\n",
    "    \"weight\"\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "print(\"Final Output DataFrame (df_out):\")\n",
    "df_out.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Exercise for Interview 00",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
